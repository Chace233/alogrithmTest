package webcollector;


import cn.edu.hfut.dmic.contentextractor.ContentExtractor;
import cn.edu.hfut.dmic.contentextractor.News;
import cn.edu.hfut.dmic.webcollector.model.CrawlDatums;
import cn.edu.hfut.dmic.webcollector.model.Page;
import cn.edu.hfut.dmic.webcollector.plugin.berkeley.BreadthCrawler;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;

import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;

public class KrCrawler extends BreadthCrawler {
    public KrCrawler(String crawlPath, boolean autoParse) {
	    super(crawlPath, autoParse);
	    this.addSeed("http://36kr.com");
	    this.addRegex("http://36kr.com/p/.*html");
	    this.addRegex("-.*\\.(jpg|png|gif).*");
	    this.addRegex("-.*#.*");
    }

    @Override
    public void visit(Page page, CrawlDatums next) {
    	String url = page.getUrl();
    	//System.out.println("URL: "+url);
    	try {
			writefile(page.getUrl());
		} catch (IOException e) {
			e.printStackTrace();
		}
    }
    
	public  void writefile(String str) throws IOException{
		File file = new File("D://content.txt");
		FileWriter fw = new FileWriter(file);
		BufferedWriter bw = new BufferedWriter(fw);
		bw.write(str);
		bw.close();
		
	}

    public static void main(String[] args) throws Exception {
    	KrCrawler crawler = new KrCrawler("crawl", true);
	    crawler.setThreads(50);
	    crawler.setTopN(1);
	    crawler.start(4);
    }

}

