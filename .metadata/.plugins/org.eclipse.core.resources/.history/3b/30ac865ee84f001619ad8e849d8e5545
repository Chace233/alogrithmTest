package webcollector;		


import cn.edu.hfut.dmic.webcollector.model.CrawlDatums;
import cn.edu.hfut.dmic.webcollector.model.Page;
import cn.edu.hfut.dmic.webcollector.plugin.berkeley.BreadthCrawler;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.net.URL;
import java.net.URLConnection;
import java.util.ArrayList;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;

public class KrCrawler2 extends BreadthCrawler {
    public KrCrawler2(String crawlPath, boolean autoParse) {
	    super(crawlPath, autoParse);
	    this.addSeed("http://36kr.com");
	    this.addRegex("http://36kr.com/p/.*html");
	    this.addRegex("-.*\\.(jpg|png|gif).*");
	    this.addRegex("-.*#.*");
    }

    @Override
    public void visit(Page page, CrawlDatums next) {
    	ArrayList<String> list = getUrls(page.getHtml(), "http://36kr.com/p/.*?html");
    	System.out.println(list);
    	for(String url : list){
    		try {
				JsoupConnect(url);
			} catch (IOException e) {
				e.printStackTrace();
			}
    	}
    	
    }
    
    public void JsoupConnect(String url) throws IOException{
    	Document doc = null;
    	String res = htmlunitTest.preloading(url);
    	doc = Jsoup.parse(res);
    	String title = doc.select("h1").text();
    	System.out.println("title: "+title);
    	String auther = doc.select("span.name").text();
    	System.out.println("auther: "+auther);
    	//��ȡͼƬ
    	Elements imgs = doc.select("div[class=mobile_article] img"); //�ҵ� <img ��ǩ
    	for(Element img : imgs){
    		String src = img.attr("src");
    		if(RegexUrl(src, "http://.*heading")){  //判断是否是文章内的图片（文章内的图片都是以!heading结尾）
    			//System.out.println("src: "+src);
    			downloadImg(src);
    		}
    	}
    }
    
    private void downloadImg(String src) throws IOException{ //����ͼƬ�ķ���
    	String fileName = getFileName(src);
    	URL url = new URL(src);
    	URLConnection uc = url.openConnection();
    	InputStream in = uc.getInputStream();
    	File file = new File("download/"+fileName);
    	FileOutputStream out = new FileOutputStream(file);
    	int i = 0;
    	while ((i = in.read()) != -1){
    		out.write(i);
    	}
    	in.close();
    }
    
    private String getFileName(String src){  //��ȡ�ļ����ķ���������ļ���������  ����Ը���������  src��ʾͼƬ��url��
    	String fileName = "";
    	int a = 0,b = 0;
    	for(int i = src.length()-1; i >=0 ;i --){
    		if(src.charAt(i) == '!') b = i;
    		if(src.charAt(i) == '/'){
    			a = i;
    			break;
    		}
    	}
    	fileName = src.substring(a, b);
    	return fileName;
    }

    private boolean RegexUrl(String url, String regex){  
    	Pattern pattern = Pattern.compile(regex);
    	Matcher m = pattern.matcher(url);
    	if(m.find()) return true;
    	return false;
    }
    
    public ArrayList<String> getUrls(String html, String regex){
    	ArrayList<String> list = new ArrayList<String>();
    	Pattern pattern = Pattern.compile(regex);
    	Matcher matcher = pattern.matcher(html);
    	while(matcher.find()){
    		list.add(matcher.group());
    	}
    	return list;
    }
    

    public static void main(String[] args) throws Exception {
    	KrCrawler2 crawler = new KrCrawler2("crawl", true);
	    crawler.setThreads(50);
	    crawler.setTopN(20);
	    crawler.start(4);
    }

}

