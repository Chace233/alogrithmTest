package webcollector;

import org.jsoup.nodes.Document;

import cn.edu.hfut.dmic.webcollector.model.CrawlDatums;
import cn.edu.hfut.dmic.webcollector.model.Page;
import cn.edu.hfut.dmic.webcollector.plugin.berkeley.BreadthCrawler;

/*public class crawlerOnePage extends BreadthCrawler{
	
	
	public crawlerOnePage(String crawlPath, boolean autoParse) {
		super(crawlPath, autoParse);
	}

	@Override
	public void visit(Page page, CrawlDatums next) {
		System.out.println("######"+page.getUrl());
		Document doc = page.getDoc();
		String name = doc.select("div[id=prod-right-side] h1").text();
		System.out.println("##name: "+name);
	}
	
	public static void onePage(String url) throws Exception{
		adafruitCrawler crawler = new adafruitCrawler("crawler", true);
		crawler.addSeed(url);
		crawler.setThreads(1);
	    crawler.setTopN(1);
	    crawler.start(1);
	}

	public static void main(String[] args) throws Exception{
		//onePage("https://www.adafruit.com/products/2167");
		adafruitCrawler crawler = new adafruitCrawler("crawler", true);
		crawler.addSeed("https://www.adafruit.com/products/2167");
		crawler.addRegex("http://www.adafruit.com/products/2167");
		crawler.addRegex("-.*\\.(jpg|png|gif).*");
		crawler.addRegex("-.*#.*"); crawler.setThreads(50);
	    crawler.setTopN(100);
	    crawler.start(4);
	}
}*/
import cn.edu.hfut.dmic.webcollector.model.CrawlDatums;
import cn.edu.hfut.dmic.webcollector.model.Page;
import cn.edu.hfut.dmic.webcollector.plugin.berkeley.BreadthCrawler;
import org.jsoup.nodes.Document;

/**
 * Crawling news from hfut news
 *
 * @author hu
 */
public class crawlerOnePage extends BreadthCrawler {

    /**
     * @param crawlPath crawlPath is the path of the directory which maintains
     * information of this crawler
     * @param autoParse if autoParse is true,BreadthCrawler will auto extract
     * links which match regex rules from pag
     */
    public crawlerOnePage(String crawlPath, boolean autoParse) {
        super(crawlPath, autoParse);

    }

    @Override
    public void visit(Page page, CrawlDatums next) {
        String url = page.getUrl();
        /*if page is news page*/
        if (page.matchUrl(url)) {
            Document doc = page.getDoc();
            String title = page.select("div[class=affix-top] h1").text();

            System.out.println("URL:\n" + url);
            System.out.println("title:\n" + title);
        }
    }

    public static void onePage(String url) throws Exception{
    	crawlerOnePage crawler = new crawlerOnePage("crawl", true);
    	crawler.addSeed(url);
    	crawler.addRegex(url);
    	crawler.addRegex("-.*\\.(jpg|png|gif).*");
    	crawler.addRegex("-.*#.*");
        crawler.setThreads(50);
        crawler.setTopN(100);
        crawler.start(4);
    }
    public static void main(String[] args) throws Exception {
    	crawlerOnePage.onePage("https://www.adafruit.com/products/2167");
    }

}
