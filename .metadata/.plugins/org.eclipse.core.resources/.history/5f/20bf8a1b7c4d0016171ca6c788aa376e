package webcollector;

import cn.edu.hfut.dmic.webcollector.model.CrawlDatums;
import cn.edu.hfut.dmic.webcollector.model.Page;
import cn.edu.hfut.dmic.webcollector.plugin.berkeley.BreadthCrawler;

public class techcrunchCrawler extends BreadthCrawler{

	public techcrunchCrawler(String crawlPath, boolean autoParse) {
		super(crawlPath, autoParse);
	}

	@Override
	public void visit(Page page, CrawlDatums next) {
		String url = page.getUrl();
		if(page.matchUrl("https://techcrunch.com/.*")){
			String title = page.select("h1").text();
			System.out.println("title: "+title);
			String content = page.select("div.article-entry text").text();
		}
		
	}
	
	public static void main(String[] args) throws Exception{
		techcrunchCrawler crawler = new techcrunchCrawler("crawl",true);
		crawler.addSeed("https://techcrunch.com/");
		crawler.addRegex("https://techcrunch.com/.*");
		crawler.addRegex("-.*\\.(jpg|png|gif).*");
		crawler.addRegex("-.*#.*");
		
		crawler.setThreads(50);
		crawler.setTopN(100);
		crawler.start(5);
	}

}
