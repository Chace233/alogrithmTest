package webcollector;

import org.jsoup.nodes.Document;

import cn.edu.hfut.dmic.webcollector.model.CrawlDatums;
import cn.edu.hfut.dmic.webcollector.model.Page;
import cn.edu.hfut.dmic.webcollector.plugin.berkeley.BreadthCrawler;

public class crawlerOnePage extends BreadthCrawler{
	
	
	public crawlerOnePage(String crawlPath, boolean autoParse) {
		super(crawlPath, autoParse);
	}

	@Override
	public void visit(Page page, CrawlDatums next) {
		System.out.println("######"+page.getUrl());
		Document doc = page.getDoc();
		String name = doc.select("div[id=prod-right-side] h1").text();
		System.out.println("##name: "+name);
	}
	
	public static void onePage(String url) throws Exception{
		adafruitCrawler crawler = new adafruitCrawler("crawler", true);
		crawler.addSeed(url);
		crawler.setThreads(1);
	    crawler.setTopN(1);
	    crawler.start(1);
	}

	public static void main(String[] args) throws Exception{
		//onePage("https://www.adafruit.com/products/2167");
		adafruitCrawler crawler = new adafruitCrawler("crawler", true);
		crawler.addSeed("https://www.adafruit.com/products/2167");
	    crawler.setThreads(50);
	    crawler.setTopN(100);
	    crawler.start(1);
	}
}
